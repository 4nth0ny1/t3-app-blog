// import type { MetadataRoute } from "next";

// export default function robots(): MetadataRoute.Robots {
//   return {
//     rules: {
//       userAgent: "*",
//       allow: "/", // allows all pages to be crawled by google
//       disallow: ["/admin", "/private/"], // disallows certain pages to be crawled by google
//     },
//     sitemap: "https://acme.com/sitemap.xml", // TODO: change when domain is ready
//   };
// }

// // you can also go the file of exact page and add robots field to the metadata hash to allow crawler to follow the links and then not index the page.
